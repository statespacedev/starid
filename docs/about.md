[1990](#1990), [2003](#2003), [2016](#2016)


*1990, [photos](https://goo.gl/photos/1VQUCGgC6ukjmpJN7) and [pdf](papers/1986%20hemenway.pdf), [pdf](papers/1986%20pierce.pdf), [pdf](papers/1987%20hemenway.pdf)*
 
In the summer of 1990 I was an astronomy undergrad at the University of Texas in Austin. In recent months the Berlin Wall had fallen and the Hubble Space Telescope had finally reached orbit. I was living just north of campus. Monthly rent was less than two hundred dollars and my landlady Martha Ann Zively, an eighty-three year old local-legend, lived overhead. Mobile phones, notebook computers, and the web were all still somewhere over the horizon. 

The previous fall, I’d started working as a research assistant with the Hubble Space Telescope Astrometry Team. This group had members from the Astronomy Department, McDonald Observatory, the Aerospace Engineering Department, and the Center for Space Research. My supervisor was Paul Hemenway, an astronomer involved with all of these organizations, and with the European Space Agency and its Hipparcos project. Paul started off by explaining the group’s projects and where I might be able to help. 

Hubble was designed for very exact and stable pointing to minimize motion smear in its images. Three optical interferometers were mounted on robotic arms in the Hubble’s focal plane to provide feedback to the pointing control system. These fine guidance sensors were a cutting-edge solution given 70s and 80s technology, with its uneasy mix of the analog and digital eras, and exact calibrations were needed on-orbit to make the whole complex system work as intended.

We used asteroids. McDonald Observatory and CSR efforts for Hubble FGS calibration became a component of the Texas Minor Planet Project. With group members in the astronomy and aerospace departments, we refined asteroid orbit determinations to the point where predicted positions and motions were exact enough to use as ground-truth references for comparison with Hubble observations. Our primary tools were a DEC VAX/VMS Cluster at CSR, the main astronomy department VAX running BSD UNIX, various early Sun workstations, and the eighty-two inch telescope at McDonald. It could image asteroids onto glass photographic plates, given enough time. For the long exposures needed to gather enough light from these dim objects, we depended on a Data General NOVA minicomputer and the suitcase sized Cassegrain Camera.

This was another sophisticated piece of custom-made analog hardware. For locking on to and tracking a guide star, it used an image dissector, a large photomultiplier tube attached to the camera’s side that looked a bit like an engine part. Its circular field of view was divided into four quadrants. A guide star was manually positioned at the center and the closed-loop control system was activated. Every second, with a loud mechanical knock, the system would bodily adjust the camera position and try to keep the guide star at the center. On a green phosphorescent screen, a fuzzy spot bounced about, now nearer the center of the cross hairs, now further out.
 
An observing night began a few hours before sunset. Down in the control room, circling the building beneath the telescope floor, the minicomputer and its control programs had to be started. One could step out onto a catwalk at the base of the dome for a spectacular view of the shadows growing out across the high desert from the mountains. Various obsolete computers and mysterious bits of hardware crowded the space. The curving walls, low ceiling, red LEDs, and glowing CRT terminals completed the 60s sci-fi movie setting.

A stack of white envelopes, each containing a glass photographic plate, was waiting. To prepare, we worked in the control room using a command line program on the NOVA to generate telescope pointing information for a list of asteroids. Using this mini may have been my last serious contact with the large eight-inch floppy disks. They were a vanishing breed by the late 80s. After jotting down notes for the planned observations, we took the plates up into the dome, where it was pitch black except for clouds of stars in the open slit. The telescope loomed overhead in the darkness and we cautiously climbed the stairs up onto the circular telescope floor. It rose and descended in order to stay near the camera as the telescope and dome were moved and one could easily step off high above the dome floor.

We positioned the floor so the camera hung at eye level. Sliding out the plate-cover opened a rectangular frame of stars, with the silhouette of the telescope secondary mirror housing and its support struts high above. McDonald maintenance staff had mounted the camera to the telescope and connected power cables. Fine tuning was always needed, and the telescope itself had to be focused. This meant adjusting the position of the secondary mirror within its housing. A rocker switch on the telescope hand controller activated a motor to move the secondary inward or outward. The exact determination of focus was old-school, using a knife-edge.

In the telescope’s focal plane, all of the light from a star converges through a single point. When a knife-edge cuts through that point, the light from the star is cut off instantly. If the knife-edge dims the star gradually, then the secondary mirror position needs to be adjusted. We wanted the point of instant cut off to be where our photographic plates were held by the camera. We fastened into the camera a special metal frame mounting a straight knife-edge, then adjusted the secondary mirror position while watching the cut off of bright stars. If there was a bit of spare time, the metal frame could be replaced with another holding the eyepiece, a heavy glass lens requiring both hands to lift. Peering inside, one saw directly a mysterious world of red or green nebulas or spiraling galaxies...

Once the telescope was ready, we could prepare the camera. With the small field of view of the telescope, asteroids moved fast relative to the sky over an interval of around ten minutes. Each asteroid was a bit different, and various orbital characteristics had to be taken into account. The direction and rate had already been computed bz the NOVA, and now the camera body was rotated in its mounting and programmed to move at a rate such that the target would appear to be motionless.
 
The rear-surface of the image dissector was a round CRT screen divided into four quadrants. Light from a star, cascading down through the photomultiplier tube, formed a green glow on the screen. We found a guide-star near the target and centered it in the screen. With the tracking loop activate, the camera position was updated once per second with a mechanical click, keeping the star at the center of the screen.

An asteroid exposure began with guide-star tracking. Then the steady clicking of the control loop would go silent for a period. The sky would turn while asteroid-light built up a spot on the photograph. Then the clicking would resume. The result was a dumbbell shape for stars, with two circular peaks connected by a trail. The asteroid was a trail with a circular peak at its midpoint. These peaks and trails became visible the next day when we developed the plates. Each had many dumbbell shaped stellar trails - short or long, thick or thin - and at the center a single UFO shaped asteroid-image.

The next steps were extracting information from the plates, and improving knowledge of the asteroid orbits. These took place back in Austin, where the Center for Space Research and Department of Aerospace Engineering become involved. Their expertise in orbit determination played an important role in the Hubble Astrometry Team. The Space Age was roughly thirty years old, and its first generation led the Center for Space Research - Ray Duncombe, Byron Tapley, and Bob Schutz.

First the plates had to be measured using a scanner and minicomputer in the scanning room, hidden behind the Astronomy Department library on the thirteenth floor of Robert Lee Moore hall - better known simply as RLM. I spent many hours in the scanning room. It was a meditative kind of place, cool and dark, with a steady drone from the electronics fans. The long back wall was covered with cabinets containing thousands of glass plates, including historic sets of survey plates from Palomar and the European Southern Observatory, alongside many plates from McDonald. Black plastic sheets shielded the end of the room from stray light, and at the center of this cave sat the PDS Microdensitometer.
 
This was a machine for mechanically scanning photographs - an interesting time capsule of analog-era technology. Light from a bulb was focused into a beam downward through a mechanically driven stage with position encoders. A photometer below the stage measured the transmitted intensity while the stage moved in a raster pattern. Sampling of the photometer and encoders was done by a very early, mini-fridge sized and rack-mounted SUN workstation.

My first observing run at McDonald was in May or June of ninety. Chat among the astronomers was about problems with Hubble that were repeatedly making headline news. I remember clearly there was still lots of discussion of the high gain antennas, because news of the catastrophic error in the primary-mirror hadn’t yet leaked out. Overhearing the veterans during those days at McDonald was an early revelation about the realities of science and technology. 

Paul and I had made the eight-hour drive to West Texas. We spent three or four nights making plates with the eighty-two inch, and then made the drive back to Austin. Texas summer heat was just beginning to get intense, and after our return I made the sweltering walk over to RLM and happily settled into the cool darkness of the scanning room. My little apartment was already uncomfortably warm during the day, even with the air conditioning running.
 
Our plates from McDonald were roughly the size and shape of writing paper. The glass was fairly thin, and fragile enough that taking extra care was natural. Held up against a background light, the star and asteroid trails were small dark smudges. With the plate secured to the PDS scanning stage, and looking across the plate’s surface, one could see the dull black trails of photographic emulsion on the surface of the glass. The control software on the workstation had to be told what areas on the plate to scan. This meant moving the scanning beam about the plate, manually steering the stage and noting coordinates.
 
At the top of the PDS, roughly at eye level, was a circular glass screen showing a magnified image of the plate illuminated by the scanning beam. This was essentially a microscope projecting directly onto the screen. Individual grains of photographic emulsion were visible, and when the beam was near a star trail it appeared as a fuzzy black worm. The stage was adjusted using two finely geared knobs, and the coordinates of the scanning beam were shown by two sets of red LEDs on the PDS console. The corners of a rectangle about a star trail were the coordinates for a raster scan, and were entered in manually at the workstation keyboard.
 
The workstation was a tall rack standing in the back corner and mounting a mini-fridge sized early SUN box. On a table beside the rack was an extremely heavy CRT monitor showing one of the first primitive UNIX GUIs I got to know well, the SunView precursor to X Window. It already had the slightly antiquated feel of an earlier era. A scanning session meant creating a set of digitized raster files, one file for each trail scanned by the pds, archived on 9-track half-inch tape. A group of files, say thirty to fifty for a plate with a good exposure and lots of stars, was created in the filesystem of the workstation and then written to tape using its sibling above on the sixteenth floor, which had the tape drive. The shift over the border from analog to digital took place in the 70s style electronics connecting the PDS to the workstation.
 
A few days after scanning those first plates, I went to meet with Paul and Ray Duncombe in WRW, the Aerospace building. I can clearly remember stopping in the Texas sun. Overhead was the typical hard blue summer sky with little white clouds, and I was already sweating just seconds after stepping outside. Exactly which stars were on the plates? How could we identify them in order to determine the position of the asteroid? Was there a program on the astronomy or aerospace computers to do this? The answer was no. There wasn’t an easy or obvious solution. Helping to figure out a practical method for our particular plates was part of my job. Not that an undergrad was expected to be able to solve the problem, but at least to get a feel for the questions involved. How did one go about recognizing stars? Humans could do it, but could an 80s computer system?
 
<a name="2003"/>*2003, [photos](https://goo.gl/photos/TbwSuETNgM9b85AF7)*
 
Thirteen years later, I entered the Aerospace graduate program and went to work in the ICESat group at CSR. Bob Schutz was my boss for the next eleven years. My job concerned star trackers - modern descendents of maritime sextants for celestial navigation - and inertial sensors. Once again I was dealing with images containing a scattering of unknown stars. Within aerospace, it’s a classic problem with a memorable name - the Lost In Space problem. Given an image of some stars, what are we looking at? Aerospace has its own perspectives, cultural bents, and tools. Astronomers don’t generally think in terms of three-dimensional unit vectors, rotation matrices, quaternions, and vector-matrix notation. It was soon apparent that the concerns and methods in aerospace were more widely applicable than those in astronomy - bringing together optimization, control, data fusion, high performance computing, and machine learning to solve real-world problems. 
 
Within weeks of beginning, star identification was again one of my major concerns. Once again the first question was whether a practical solution was available. I checked back with people in the astronomy department, after being out of touch for nine years or so. Pete Shelus from the Hubble astrometry days was a member of our CSR group and pointed me in the right directions. There was a strong sense of continuity - here was a problem that really needed addressing. The obvious differences were that computing hardware was now more powerful, and digital imaging was now standard. There was no longer an analog to digital divide to cross, everything was in binary.
 
ICESat’s control system usually made it straightforward to predict which stars each image contained. This wasn’t obvious or straightforward at first and it took effort and thought to really understand the data coming from the spacecraft. There were four star imagers of three different hardware-types onboard, all sampling at ten hertz or more. These were classic 80s star trackers and did not provide star identifications. There was also higher-frequency angular-rate data from the inertial unit.
 
A pointing vector could be estimated for each star-image, and it was usually enough to check whether star-images with appropriate brightnesses were near their predicted positions. Brightness information tends to muddy the star identification problem because it’s difficult to either measure or predict for a particular imager. Images have better geometric information than brightness information - an astronomer interested in brightness does photometry with dedicated sensors, not with imagers.
 
An additional check was that angles between observed star pairs matched predictions, and one of my first objectives was to model errors in these angles from flight data. Focusing on star pairs is a big step in the direction of looking at star triangles and patterns. I quickly began exploring the literature on star identification and related topics - and discovered a self-contained little intellectual world. 
 
Its roots go back to ancient celestial navigation. The technology has evolved continuously from the Age of Sail. In the Second World War many large aircraft had a bubble window facing upward for a navigator to make stellar observations. After the war, computing and imaging brought increasing automation. The Cold War created an enormous propulsive force behind the technology. Many people became uneasily aware of guidance systems. While most of the funding ended up in integrated circuits and inertial guidance sensors, automated star tracking quietly matured in parallel. Star trackers are critical for spacecraft, and are still used on certain high altitude aircraft. The classical period, when the fundamental concepts were sketched out, was the sixties through the eighties - the High Cold War era - with many scientists switching over into industry as direct government funding decreased. 
 
It soon became clear that there were still no easily-available solutions for the star identification problem. Apparently, each time star identification software had been developed, it’s been for a classified or industry project. If you were serious about star identification, you probably wanted to sell star trackers. That’s a mature industry now, with plenty of sellers and not a lot of buyers - there’s little motivation to think that way anymore, and I was soon meeting with the university intellectual property office concerning open source licensing. 
 
<a name="2016"/>*2016, [photos](https://goo.gl/photos/gSWh1w6pUt8VUnbN7)*
 
Another thirteen years passed. Excitement was growing about advances in neural networks, especially at Google, which had just open sourced Tensorflow. For a number of reasons, it was clearly time to tackle the problem directly, using both geometric and machine learning methods in parallel. 
 
The concept was to start from scratch as a Github open source project, integrating Tensorflow from the beginning. This meant working in C++ Eigen and Python Numpy. The only external input was to be a list of star positions. NASA’s Skymap star catalog was an ideal source. Skymap was created in the 90s specifically for use with star trackers. We’d used it extensively for ICESat, even collaborating where possible with its creators. When Hubble was launched, one its early problems was bad guide stars. As part of the overall Hubble recovery effort, NASA pushed Skymap forward as an improved description of the sky as seen by standard star trackers. 
 
Skymap is simply a list of star positions, so how does one generate a star image? The core problem is searching for neighbors of an arbitrary point on a sphere. For example, given a list of points on Earth, which of the points are near a particular latitude and longitude? The usual answers involve dividing the sphere up into tiles, transforming and subdividing, etc. Even a square-sky is not unheard of. A more dynamic and flexible approach was published by Daniele Mortari. It’s closely related to lookup and hash tables, but has some unique and interesting quirks.
 
View stars as unit vectors with three coordinates between -1.0 and +1.0. We’re searching for stars within small ranges of each coordinate. Picture three thin rings on the sky, one centering on each coordinate-axis, and finding the stars inside the small region where the rings intersect. We’re left with three independent searches for small ranges of values, followed by an intersection of the results. Each search is performed on a separate precomputed key-value table, with sorted keys from -1.0 to +1.0 and values representing star labels. Performance can be improved by fitting a curve to the sorted floating-point keys and using it to calculate the low and high indexes into the table, creating something like a ranged-search hash-table with the fitted curve acting as a hash function.
 
Cultural differences between machine learning and aerospace became apparent. To oversimplify, machine learning wants to be about images, while aerospace wants to be about three dimensional unit vectors. Images appear more practical in many contexts, unit vectors are ideal geometrically. Over roughly eight months, a higher-level image interface organically grew over the lower-level unit vector geometry, and a curious sequence of coincidences took place. 

Standard 90s star tracker images were eight degrees or 28,000 arcseconds per side - roughly sixteen times the apparent diameter of the moon. The hello world problem in machine learning, MNIST, was standardized in the late 90s using data files and images with 28 pixels per side. Adopting these standards resulted in star images with thousand-arcsecond pixels. This all happened quite accidentally. At first, actual MNIST data files were simply overwritten with star images and then fed into standard machine learning processors. Gradually advantages became apparent, beyond data file format compatibility. The implications are deeper than nice rounding properties, since they effectively mean low resolution - at the level of a toy camera or blurry mobile phone photo. By comparison, real star tracker images can involve sub-arcsecond resolutions. 

Low resolution makes the star identification problem more interesting. You’re forced to use global structures and patterns within an image, rather than localized features and heuristics. There’s simply less information available and you have to be more thoughtful about using it, even suggesting questions about how the human brain solves the problem. For example, a typical high-resolution aerospace algorithm might focus on the exact distance between a pair of stars, along with the angle to a third star. That’s clearly not how the brain identifies stars, but what is the brain in fact doing?

Another nice coincidence is simple once you see it, but isn’t so obvious at first. When you want to identify a particular star in an image, it helps to shift the star to the image-center and make its presence implicit. There’s no point in including it in the image, what’s significant is the geometry of the other stars. It becomes the origin of the coordinate system, and if there’s another star nearby, as often happens in a low resolution image, there’s no confusion. In practice, the effects are even nicer than you might think. In a way, your getting an extra star for free while eliminating annoying coordinate transformations. 

All the way back to 1990, it was clear that the shapes of triangles formed by a star field can be used to identify the stars. A few minutes thought always gave the feeling that this is somehow an iterative and even recursive approach. Once you start thinking about triangles, they tend to multiply, which seemed uncomfortable. Where does it end? Skipping ahead to the answer, enlightenment came once the problem was stated as simply as possible. Start with a set of star identities and iteratively set aside those that can’t be correct until only one remains. It’s brute force and hopefully there will be time later to find deeper insights. The main thing is, it works.

Between the star-level and triangle-level is the pair-level. It’s the fundamental structural unit. Soon after code for star images came code for pairs separated by less than eleven degrees on the sky. This was the fourth use of the key-value table described above, to represent nearly one million pairs as angles and member star identifiers.

The initial concept was to focus on groups of four stars instead of just three. For a triangle of three stars, adding a fourth provides significantly more information. You go from three edges to six, two of which are a shared pair. The tradeoff is significantly more complexity. For two adjacent triangles, the shared-pair represents a new type of constraint for which stars are possible. Picture two sets of possible stars for the two triangles, kept in agreement via the shared-pair. With low resolution, this is harder than it sounds. There are too many pairs that meet low resolution constraints. A low resolution shared-pair just doesn’t provide enough unique information. It’s too ambiguous. In other words, at low resolution many of the skies triangles are similar.

Eventually, the concept of the shared-pair became the focus. Any pair of stars can be a shared-pair parent with many child-triangles. With the target star implicit in the center of an image containing ten other stars, there are ten shared-pairs that include the target star. Each of these is the parent of nine child-triangles.

